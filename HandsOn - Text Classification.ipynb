{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2e2ef-d104-4c6f-b8dd-aafbd8414c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets peft evaluate pandas scikit-learn xformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b51973-79d2-420c-bc07-c8eff7c68365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    RobertaTokenizerFast,\n",
    "    RobertaForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoConfig,\n",
    "    pipeline\n",
    ")\n",
    "from evaluate import evaluator\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
    "import peft\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3674c7b1-649c-4df0-8b12-ce5c6a979d97",
   "metadata": {},
   "source": [
    "# Demystifying Chat-GPT\n",
    "## Use-Case: Text Classification\n",
    "Text Classification is the task of assigning a label or class to a given text. Some use cases are sentiment analysis, natural language inference, and assessing grammatical correctness.\n",
    "\n",
    "LLMs, which are capable to perform text classification tasks are in general explicitly trained on the requested labels.\n",
    "\n",
    "E.g. the model Roberta-base_ag_newsl is a fine-tuned version of roberta-base on the ag_news datase and can map the text to the classes \n",
    "\n",
    "- World\n",
    "- Sports\n",
    "- Business\n",
    "- Sci/Tech\n",
    "\n",
    "We can easily setup this model and use it to assign a given text or headline to the above classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14312011-ea24-4992-8143-b119d6a2a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use such a model you can simply set it up\n",
    "classifier = pipeline('text-classification','achimoraites/roberta-base_ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd7fe6-b85b-45a5-854b-eadd7cba4dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We take some examples from the ag_news dataset and let the model classify them\n",
    "news = [\"Talks End With No U.S. Climate Deal A U.N. conference ended early Saturday with a vague plan for informal new talks on how to slow global warming but without a U.S. commitment to multilateral negotiations on next steps, including emissions controls.\",\n",
    "        \"Texas' Johnson, Benson Go Out With Win (AP) AP - Their final games will be remembered for the plays others made. Still, Texas tailback Cedric Benson and linebacker Derrick Johnson went out the way they wanted to: with a Rose Bowl win.\",\n",
    "        \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\band of ultra-cynics, are seeing green again.\",\n",
    "        \"Gene Blocker Turns Monkeys Into Workaholics - Study (Reuters) Reuters - Procrastinating monkeys were turned\\into workaholics using a gene treatment to block a key brain\\compound, U.S. researchers reported on Wednesday.\"]\n",
    "for nw in news:\n",
    "    r = classifier(nw)\n",
    "    print(f\"Text to classify: {nw}\")\n",
    "    print(f\"Classification result:{r}\")\n",
    "    print(\"--------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fffbc1-6638-4250-a38b-afb5e5d3a52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# But we can also take a more current headline, e.g. from cnn (https://edition.cnn.com/)\n",
    "text = \"India becomes the fourth country ever to land a spacecraft on the moon\"\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ba9f6f-6b4f-4739-bd91-8a6cc092f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Laulauga Tausaga-Collins wins US’ first women’s discus world championship\"\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04a63e-e0f9-4555-b389-f533558a0c1c",
   "metadata": {},
   "source": [
    "## Text Classification in Automotive Industry\n",
    "\n",
    "Field data analysis in the automotive industry is crucial for several reasons, primarily centered around improving vehicle design, performance, safety, and overall customer satisfaction. One of the most important statistics is the distribution on the failure modes of the vehicles. The main failure modes are:\n",
    "\n",
    "- **Engine and Transmission Failures**: Problems with the engine or transmission can lead to significant performance issues, reduced fuel efficiency, and even complete breakdowns.\n",
    "- **Electrical System Failures**: Malfunctions in the electrical system can cause issues with lights, sensors, controls, and other critical components.\n",
    "- **Brake System Failures**: Brake failure can be extremely dangerous, compromising vehicle safety and leading to accidents.\n",
    "- **Suspension and Steering Failures**: Faulty suspension or steering systems can cause handling problems and compromise the vehicle's stability.\n",
    "- **Airbag and Safety System Failures**: Failure of airbags and other safety systems can result in severe injuries during accidents.\n",
    "- **Fuel System Failures**: Issues with the fuel system can lead to fuel leaks, reduced efficiency, and even fires.\n",
    "- **Exhaust System Failures**: Problems in the exhaust system can result in increased emissions, decreased performance, and environmental issues.\n",
    "- **Cooling System Failures**: Failure of the cooling system can lead to engine overheating and potential damage.\n",
    "- **Tire Failures**: Defective or worn-out tires can result in blowouts and accidents.\n",
    "- **Electronic Component Failures**: Problems with electronic components, such as the infotainment system or computer modules, can lead to malfunctions and inconvenience for the driver.\n",
    "\n",
    "\n",
    "These information can be gathered from customer and/or technician comments, but they are stored in unstructured text data, e.g.:\n",
    "\n",
    "- The exhaust system on my car started to rattle and vibrate while driving - Exhaust System Failures\n",
    "- The engine failed due to a faulty fuel pump. - Engine and Transmission Failures\n",
    "\n",
    "We have a dataset containing customer/technician comments with according labels prepared. Let's take a look onto it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cba393-dc7b-469e-9970-864e66d5e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_parquet('automotive_failure_mode_comments.parquet')\n",
    "idx = [624, 18] + random.choices(df.index, k=10)\n",
    "df.loc[idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07889bdd-a5a5-4086-853a-42a044a6f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are interessted into the failure mode labels, we simply take the ground truth labels from the dataset\n",
    "failure_mode_labels = df[\"ground_truth\"].unique()\n",
    "print(failure_mode_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c7ab0a-a083-462f-8eab-0a792dcd4974",
   "metadata": {},
   "source": [
    "## Custom Text Classification with RoBERTa\n",
    "\n",
    "The approach to use a text classification model is obvious. However, the main problem is that such models have no knowledge of custom data and must first be trained on them.\n",
    "\n",
    "We will use the model RoBERTa (Robust optimized BERT approach), which is based on Google’s BERT model released in 2018. RoBERTa is a transformer model (123 million parameters) pretrained on a large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb890b-0dc6-4b47-9703-1e207c57ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without training, RoBERTa is just guessing some labels\n",
    "\n",
    "model_id = \"roberta-base\"\n",
    "base_pipe = pipeline(\"text-classification\", model=model_id, device=0)\n",
    "text = \"The engine failed due to a faulty fuel pump.\"\n",
    "base_pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111d639-d975-4e3a-9ea7-2a4226048a14",
   "metadata": {},
   "source": [
    "# Fine Tune RoBERTa\n",
    "\n",
    "We can see on the response of the model evaluation... the model itself told us, that we need to train it!!!\n",
    "\n",
    "Let's do it and jump to the notebook __HandsOn - Fine Tuning__\n",
    "\n",
    "If you don't want to fine-tune the model right now, the model is already available in the tar archive __roberta-base-fine-tuned__\n",
    "\n",
    "To extract it, go to the launcher --> terminal --> type __tar -xf roberta-base-fine-tuned.tar__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0082e98a-0362-425b-9868-fa5c65361197",
   "metadata": {},
   "source": [
    "### After fine-tuning the model, we can now use it for text classification in our use-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3da05-ef33-48a3-a82b-2ca7cb35a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set use-case configs\n",
    "model_id = \"roberta-base\"\n",
    "repository_id = \"roberta-base-fine-tuned\"\n",
    "label_column = \"ground_truth\"\n",
    "\n",
    "# Prepare training and validation datasets\n",
    "# Load dataset\n",
    "df = pd.read_parquet('automotive_failure_mode_comments.parquet')\n",
    "df_fine_tuning = pd.DataFrame()\n",
    "df_fine_tuning[\"text\"] = df[\"comment\"]\n",
    "df_fine_tuning[\"label\"] = df[label_column]\n",
    "\n",
    "# Mapping from label to integer (and vice versa)\n",
    "labels = df_fine_tuning[\"label\"].unique()\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "df_fine_tuning[\"label\"] = df_fine_tuning[\"label\"].map(label2id)\n",
    "\n",
    "train_text, val_text, train_labels, val_labels = train_test_split(\n",
    "    df_fine_tuning[\"text\"].tolist(), df_fine_tuning[\"label\"].tolist(), test_size=0.2, random_state=1909\n",
    ")\n",
    "train_data = {\"text\": train_text}\n",
    "train_data[\"label\"] = train_labels\n",
    "train_dataset = Dataset.from_dict(train_data).with_format(\"torch\")\n",
    "val_data = {\"text\": val_text}\n",
    "val_data[\"label\"] = val_labels\n",
    "val_dataset = Dataset.from_dict(val_data).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bbd6e-b77f-4688-b3fa-9c344ae9e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "peft_model_id = repository_id\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "inference_model = AutoModelForSequenceClassification.from_pretrained(config.base_model_name_or_path, id2label=id2label)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# model evaluation\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "task_evaluator = evaluator(\"text-classification\")\n",
    "\n",
    "# original model\n",
    "pipe1 = pipeline(\"text-classification\", model=inference_model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "results1 = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe1,\n",
    "    data=val_dataset,\n",
    "    metric=metric,\n",
    "    label_mapping=label2id,\n",
    ")\n",
    "\n",
    "# fine-tuned model\n",
    "model_finetuned = PeftModel.from_pretrained(inference_model, peft_model_id)\n",
    "pipe2 = pipeline(\"text-classification\", tokenizer=tokenizer, model=model_finetuned, device=0)\n",
    "\n",
    "results2 = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe2,\n",
    "    data=val_dataset,\n",
    "    metric=metric,\n",
    "    label_mapping=label2id,\n",
    ")\n",
    "\n",
    "print(f\"Evaluation result for {inference_model.name_or_path}: {results1}\")\n",
    "print(f\"Evaluation result for {peft_model_id}: {results2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe97546-310d-4682-a0d8-b78199046f9f",
   "metadata": {},
   "source": [
    "## Previous evaluated model accurency\n",
    "\n",
    "Evaluation result for roberta-base: {'accuracy': 0.08, 'total_time_in_seconds': 4.596960440998373, 'samples_per_second': 43.507009156807925, 'latency_in_seconds': 0.022984802204991862}\n",
    "\n",
    "\n",
    "Evaluation result for roberta-base-fine-tuned: {'accuracy': 0.99, 'total_time_in_seconds': 4.937323142999958, 'samples_per_second': 40.50778006773897, 'latency_in_seconds': 0.024686615714999787}\n",
    "\n",
    "__Because the parameters in the fine tuning are randomly initialize, it might be that some fine-tuned models perform better than others!!!__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0a9c3-a812-4e06-93be-21adf93ef19a",
   "metadata": {},
   "source": [
    "## Zero-Shot-Classifcation\n",
    "\n",
    "However, there already exists models which can handle custom labels, e.g. bart-large-mnli (406 million parameters) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa52c0e-5d73-43c3-9c97-1e234d31663a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero-shot model\n",
    "zero_shot_pipe = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e5768-517e-4368-8a97-de4d8d94b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = \"The exhaust system on my car started to rattle and vibrate while driving\" #Exhaust System Failures\n",
    "zero_shot_pipe(c1, failure_mode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c435aa0-6111-4e6e-9d87-0fb9d4714e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = \"The engine failed due to a faulty fuel pump.\" # Engine and Transmission Failures\n",
    "zero_shot_pipe(c2, failure_mode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdcbfff-1637-4127-877f-86265e81e061",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = zero_shot_pipe(val_dataset[\"text\"], candidate_labels=failure_mode_labels)\n",
    "num_matching = 0\n",
    "for i, r in enumerate(res):\n",
    "    predicted_label = label2id[r[\"labels\"][0]]\n",
    "    gt_label = val_dataset[\"label\"][i]\n",
    "    num_matching += int(predicted_label == gt_label)\n",
    "\n",
    "print(f\"Evaluation result for zero-shot bart-large-mnli: {num_matching/len(val_dataset['text'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a88040c-dd62-422e-ad6e-8f3c811f2888",
   "metadata": {},
   "source": [
    "## Previous evaluated model accurency\n",
    "\n",
    "Evaluation result for zero-shot bart-large-mnli: 0.875"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
